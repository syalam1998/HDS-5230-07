{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "# Dataset paths\n",
        "dataset_paths = {\n",
        "    1000: \"Data_1000.csv\",\n",
        "    10000: \"Data_10000.csv\",\n",
        "    100000: \"Data_100000.csv\"\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# Loop over datasets and configurations\n",
        "for size, path in dataset_paths.items():\n",
        "    print(f\"\\nProcessing dataset size: {size}\")\n",
        "    df = pd.read_csv(path)\n",
        "    X = df.drop(columns=[\"outcome\"]).values\n",
        "    y = df[\"outcome\"].values\n",
        "\n",
        "    # Train-validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    for config in [\"1-layer\", \"2-layer\"]:\n",
        "        print(f\"  Training {config} model...\")\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Input(shape=(X.shape[1],)))\n",
        "        model.add(layers.Dense(4, activation='relu'))\n",
        "        if config == \"2-layer\":\n",
        "            model.add(layers.Dense(4, activation='relu'))\n",
        "        model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Measure execution time\n",
        "        start_time = time.time()\n",
        "        history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_val, y_val))\n",
        "        end_time = time.time()\n",
        "\n",
        "        train_acc = history.history['accuracy'][-1]\n",
        "        val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "        results.append({\n",
        "            \"Dataset Size\": size,\n",
        "            \"Model Config\": config,\n",
        "            \"Training Error\": round(1 - train_acc, 4),\n",
        "            \"Validation Error\": round(1 - val_acc, 4),\n",
        "            \"Time Taken (s)\": round(end_time - start_time, 2)\n",
        "        })\n",
        "\n",
        "# Output results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nSummary Table:\")\n",
        "print(results_df)\n",
        "\n",
        "# Optionally save to CSV\n",
        "results_df.to_csv(\"deep_learning_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B23t-5zR7Ipp",
        "outputId": "f3473510-64c2-49a5-e5fc-8d9f2f235690"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing dataset size: 1000\n",
            "  Training 1-layer model...\n",
            "  Training 2-layer model...\n",
            "\n",
            "Processing dataset size: 10000\n",
            "  Training 1-layer model...\n",
            "  Training 2-layer model...\n",
            "\n",
            "Processing dataset size: 100000\n",
            "  Training 1-layer model...\n",
            "  Training 2-layer model...\n",
            "\n",
            "Summary Table:\n",
            "   Dataset Size Model Config  Training Error  Validation Error  Time Taken (s)\n",
            "0          1000      1-layer          0.0950            0.0900            6.49\n",
            "1          1000      2-layer          0.0612            0.0700            8.18\n",
            "2         10000      1-layer          0.0034            0.0005           26.01\n",
            "3         10000      2-layer          0.0038            0.0025           25.50\n",
            "4        100000      1-layer          0.0014            0.0010          176.49\n",
            "5        100000      2-layer          0.0018            0.0018          193.81\n"
          ]
        }
      ]
    }
  ]
}